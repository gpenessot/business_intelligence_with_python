{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Practical Case: Creating a Complete ETL Pipeline\n",
    "\n",
    "## 6.4.2 Implementation with Airflow\n",
    "\n",
    "First, create ETL functions:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from loguru import logger\n",
    "import sqlite3\n",
    "\n",
    "def extract(destination_folder=\"./data/raw/\"):\n",
    "    \"\"\"Downloads the Olist dataset from Kaggle and extracts it\"\"\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    api.dataset_download_files('olistbr/brazilian-ecommerce', path=destination_folder, unzip=True)\n",
    "    logger.info(f\"Dataset downloaded to {destination_folder}\")\n",
    "    return destination_folder\n",
    "\n",
    "def clean_olist_data(df):\n",
    "    \"\"\"Performs basic cleaning on the Olist dataset\"\"\"\n",
    "    irrelevant_columns = [\n",
    "        'seller_zip_code_prefix', 'customer_zip_code_prefix', \n",
    "        'geolocation_zip_code_prefix', 'product_category_name_english'  \n",
    "    ]\n",
    "    df = df.drop(columns=irrelevant_columns, errors='ignore')\n",
    "    \n",
    "    date_columns = ['order_purchase_timestamp', 'order_approved_at', \n",
    "                   'order_delivered_carrier_date', 'order_delivered_customer_date', \n",
    "                   'order_estimated_delivery_date']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def transform(raw_data_dir=\"./data/raw\", processed_data_dir=\"./data/processed\"):\n",
    "    \"\"\"Cleans the Olist CSV files and saves the cleaned versions\"\"\"\n",
    "    files_to_clean = [\n",
    "        'olist_customers_dataset.csv',\n",
    "        'olist_geolocation_dataset.csv',\n",
    "        'olist_order_items_dataset.csv',\n",
    "        'olist_order_payments_dataset.csv',\n",
    "        'olist_order_reviews_dataset.csv',\n",
    "        'olist_orders_dataset.csv',\n",
    "        'olist_products_dataset.csv',\n",
    "        'olist_sellers_dataset.csv'\n",
    "    ]\n",
    "\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "    for file_name in files_to_clean:\n",
    "        raw_file_path = os.path.join(raw_data_dir, file_name)\n",
    "        if os.path.exists(raw_file_path):\n",
    "            logger.info(f\"Cleaning {file_name}...\")\n",
    "            try:\n",
    "                df = pd.read_csv(raw_file_path)\n",
    "                df_cleaned = clean_olist_data(df)\n",
    "                processed_file_path = os.path.join(processed_data_dir, file_name)\n",
    "                df_cleaned.to_csv(processed_file_path, index=False)\n",
    "                logger.info(f\"Cleaned file saved to {processed_file_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error cleaning {file_name}: {e}\")\n",
    "        else:\n",
    "            logger.warning(f\"File not found: {raw_file_path}\")\n",
    "    \n",
    "    return processed_data_dir\n",
    "\n",
    "def load_to_database(processed_data_dir, db_name=\"olist.db\"):\n",
    "    \"\"\"Loads the cleaned Olist data into a SQLite database\"\"\"\n",
    "    data = {}\n",
    "    for file_name in os.listdir(processed_data_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(processed_data_dir, file_name)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                table_name = file_name[:-4]\n",
    "                data[table_name] = df\n",
    "                logger.info(f\"Loaded {file_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {file_name}: {e}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_name)\n",
    "    for table_name, df in data.items():\n",
    "        try:\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            logger.info(f\"Loaded {table_name} into {db_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {table_name}: {e}\")\n",
    "    conn.close()\n",
    "```\n",
    "\n",
    "Then create the Airflow DAG:\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from etl_functions import extract, transform, load_to_database\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 7, 25),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'olist_etl_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='ETL pipeline for Olist dataset',\n",
    "    schedule=timedelta(days=1),\n",
    ")\n",
    "\n",
    "with dag:\n",
    "    extract_task = PythonOperator(\n",
    "        task_id='extract_data',\n",
    "        python_callable=extract,\n",
    "    )\n",
    "\n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_data',\n",
    "        python_callable=transform,\n",
    "    )\n",
    "\n",
    "    load_task = PythonOperator(\n",
    "        task_id='load_data',\n",
    "        python_callable=load_to_database,\n",
    "        op_kwargs={\n",
    "            'processed_data_dir': '{{ task_instance.xcom_pull(task_ids=\"transform_data\") }}',\n",
    "            'db_name': 'olist.db',\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    extract_task >> transform_task >> load_task\n",
    "```\n",
    "\n",
    "## 6.4.3 Implementation with Luigi\n",
    "\n",
    "Create the Luigi pipeline:\n",
    "\n",
    "```python\n",
    "import luigi\n",
    "from etl_functions import extract, transform, load_to_database\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class ExtractTask(luigi.Task):\n",
    "    \"\"\"Task to extract data from Kaggle.\"\"\"\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f'./data/raw/{self.date}/')\n",
    "\n",
    "    def complete(self):\n",
    "        return self.output().exists() and len(os.listdir(self.output().path)) > 0\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"Starting ExtractTask for date: {self.date}\")\n",
    "        os.makedirs(self.output().path, exist_ok=True)\n",
    "        extract(self.output().path)\n",
    "        print(\"ExtractTask completed\")\n",
    "\n",
    "class TransformTask(luigi.Task):\n",
    "    \"\"\"Task to transform the extracted data.\"\"\"\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "\n",
    "    def requires(self):\n",
    "        return ExtractTask(date=self.date)\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f'./data/processed/{self.date}/')\n",
    "\n",
    "    def complete(self):\n",
    "        return self.output().exists() and len(os.listdir(self.output().path)) > 0\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"Starting TransformTask for date: {self.date}\")\n",
    "        os.makedirs(self.output().path, exist_ok=True)\n",
    "        transform(self.input().path, self.output().path)\n",
    "        print(\"TransformTask completed\")\n",
    "\n",
    "class LoadTask(luigi.Task):\n",
    "    \"\"\"Task to load the transformed data into a SQLite database.\"\"\"\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "    db_name = luigi.Parameter(default=\"olist.db\")\n",
    "\n",
    "    def requires(self):\n",
    "        return TransformTask(date=self.date)\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f'{self.db_name}_{self.date}')\n",
    "\n",
    "    def complete(self):\n",
    "        return self.output().exists()\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"Starting LoadTask for date: {self.date}\")\n",
    "        load_to_database(self.input().path, self.output().path)\n",
    "        print(\"LoadTask completed\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    luigi.build([LoadTask()], local_scheduler=False)\n",
    "```\n",
    "\n",
    "## 6.4.4 Comparison of Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
